{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Recommender by Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to predict recommended margin range for when a customer wants to by a new product by using unsupervised learning. \n",
    "More specifically we will first cluster products and then customers. After the clustering is performed we can calcuate the upper and lower bound for recommended margin with the following formula: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the necessary libraries, and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "This section contains a variety of important functions used throughout the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficient_variation(data, feature): \n",
    "    '''\n",
    "    The function calculates the coefficient of variation (CV) of the \"Margin\" for each category in the specified feature.\n",
    "\n",
    "    Returns a sorted DataFrame with the mean, standard deviation, and CV for each category.\n",
    "    '''\n",
    "    \n",
    "    grouped_data = data.groupby(feature)\n",
    "    mean, std = grouped_data[\"Margin\"].mean(), grouped_data[\"Margin\"].std()\n",
    "    df = pd.concat([mean, std], axis = 1).reset_index()\n",
    "    df.columns = [feature, \"Mean\", \"Std\"]\n",
    "    df[\"CoefficientOfVariation\"] = (df[\"Std\"]/df[\"Mean\"])**2\n",
    "\n",
    "    return df.sort_values(by = \"CoefficientOfVariation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_data(feature, data):\n",
    "    '''\n",
    "    Splits a DataFrame into multiple DataFrames based on the unique values of a specified categorical feature.\n",
    "\n",
    "    Parameters:\n",
    "    - feature: The column name to split the DataFrame by.\n",
    "    - data: The DataFrame to split.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where keys are unique feature values and values are the corresponding DataFrames.\n",
    "    '''\n",
    "\n",
    "    categories = list(data[feature].unique())\n",
    "    dataframes = {}\n",
    "\n",
    "    for c in categories: \n",
    "        df = data[data[feature] == c]\n",
    "        dataframes[c] = df\n",
    "        \n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "'''\n",
    "This cell contains all functions that are used to create new features used by the KMeans\n",
    "algorithms to group products in clusters. \n",
    "More functions are needed to create more features to achieve better clustering results. \n",
    "'''\n",
    "\n",
    "def add_average_margin_feature(dataframe):\n",
    "    means = dataframe.groupby(\"ProductName\")[\"Margin\"].mean().reset_index()\n",
    "    means.columns = [\"ProductName\", \"MeanMargin\"]\n",
    "    new_df = pd.merge(means, dataframe, on = \"ProductName\")\n",
    "    return new_df \n",
    "    \n",
    "\n",
    "def add_average_cost_per_unit_feature(dataframe):\n",
    "    dataframe.loc[:, \"CostPerUnit\"] = dataframe[\"Cost\"]/dataframe[\"Quantity\"]\n",
    "    means = dataframe.groupby(\"ProductName\")[\"CostPerUnit\"].mean().reset_index()\n",
    "    means.columns = [\"ProductName\", \"MeanCostPerUnit\"]\n",
    "    new_df = pd.merge(means, dataframe, on = \"ProductName\")\n",
    "    return new_df \n",
    "\n",
    "def add_average_sales_in_past_per_product(n_months, dataframe):\n",
    "    dataframe[\"OrderDate\"] = pd.to_datetime(dataframe[\"OrderDate\"])\n",
    "    last_date = dataframe[\"OrderDate\"].max()\n",
    "    n_months_ago = last_date - pd.DateOffset(months = n_months)\n",
    "    filtered_data = dataframe[dataframe[\"OrderDate\"] >= n_months_ago]\n",
    "    meanSales = filtered_data.groupby(\"ProductName\")[\"Sales\"].mean().reset_index()\n",
    "    meanSales.columns = [\"ProductName\", \"AverageSalesPastMonths\"]\n",
    "    new_df = pd.merge(meanSales, dataframe, on = \"ProductName\")\n",
    "    return new_df\n",
    "\n",
    "def number_of_orders_per_product(dataframe): \n",
    "    product_order_counts = dataframe.groupby(\"ProductID\")[\"OrderID\"].count().reset_index()\n",
    "    product_order_counts = product_order_counts.rename(columns = {\"OrderID\" : \"NumberOfOrders\"})\n",
    "    dataframe = dataframe.merge(product_order_counts, on = \"ProductID\", how = \"left\")\n",
    "    return dataframe\n",
    "    \n",
    "def add_total_sales_for_product(dataframe): \n",
    "    product_sales = dataframe.groupby(\"ProductID\")[\"Sales\"].sum().reset_index()\n",
    "    product_sales.columns = [\"ProductID\", \"TotalSales\"]\n",
    "    new_df = pd.merge(dataframe, product_sales, on = \"ProductID\", how = \"left\")\n",
    "    return new_df\n",
    "\n",
    "def add_total_cost_for_product(dataframe): \n",
    "    product_cost = dataframe.groupby(\"ProductID\")[\"Cost\"].sum().reset_index()\n",
    "    product_cost.columns = [\"ProductID\", \"TotalCost\"]\n",
    "    new_df = pd.merge(dataframe, product_cost, on = \"ProductID\", how = \"left\")\n",
    "    return new_df\n",
    "\n",
    "    \n",
    "\n",
    "# Group products based on sales. Use the TotalRevenue. \n",
    "# Start by picking two features and see if there is any clustering. For example revenue and avg margin, or margin and number of orders. \n",
    "# Do not use average but sum. Also try changing the scaling method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'CogitoXInfor-h24/ECOMMRecords2020.csv'\n",
    "ecommerce_df = pd.read_csv(filepath)\n",
    "\n",
    "ecommerce_df[\"Cost\"] = (ecommerce_df[\"Sales\"] - ecommerce_df[\"Profit\"])/ecommerce_df[\"Quantity\"] \n",
    "ecommerce_df[\"Margin\"] = ecommerce_df[\"Profit\"]/ecommerce_df[\"Sales\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_preprocessing(dataframes: dict, scaler) -> dict:\n",
    "    '''\n",
    "    The function applies multiple feature engineering steps to each dataframe in the input dictionary. \n",
    "\n",
    "    It then returns a dictionary with the engineered dataframes. \n",
    "    '''\n",
    "    result = {}\n",
    "    for category in dataframes.keys():\n",
    "        df = dataframes[category]\n",
    "\n",
    "        df = number_of_orders_per_product(df)\n",
    "        df = add_total_sales_for_product(df)\n",
    "        df = add_total_cost_for_product(df)\n",
    "        df = add_average_margin_feature(df)\n",
    "\n",
    "        result[category] = df\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(dataframes : dict, engineered_features : list): \n",
    "    '''\n",
    "    Identifies the optimal number of clusters for KMeans using the elbow method and plotting the silhouette score. \n",
    "\n",
    "    Returns a plot of the interia score and silhouette score versus number of clusters for each of the dataframes \n",
    "    in the input dictionary. \n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    ks = [x for x in range(2, 11)]\n",
    "    inertia_scores = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for category in dataframes.keys(): \n",
    "        data = scaler.fit_transform(dataframes[category][engineered_features])\n",
    "        inertia = []\n",
    "        silhouette = []\n",
    "        for k in ks: \n",
    "            model = KMeans(n_clusters = k, random_state = 42, init = 'k-means++')\n",
    "            predicted = model.fit_predict(data)\n",
    "            inertia.append(model.inertia_)\n",
    "            silhouette.append(silhouette_score(data, predicted))\n",
    "\n",
    "        inertia_scores.append(inertia)\n",
    "        silhouette_scores.append(silhouette)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = len(dataframes.keys()), ncols = 2, figsize = (15, 14), squeeze=False)\n",
    "    for i in range(len(dataframes.keys())): \n",
    "        ax[i, 0].plot(ks, inertia_scores[i])\n",
    "        ax[i, 1].plot(ks, silhouette_scores[i])\n",
    "        ax[i, 0].set_xlabel(\"Number of Clusters\")\n",
    "        ax[i, 0].set_ylabel(\"Inertia\")\n",
    "        ax[i, 1].set_xlabel(\"Number of Clusters\")\n",
    "        ax[i, 1].set_ylabel(\"Silhouette Score\")\n",
    "        fig.text(0.5, 0.95 - (i*0.3), f\"{list(dataframes.keys())[i]}\", ha = 'center', fontsize = 14, weight = \"bold\")\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(model, data, title):\n",
    "    '''\n",
    "    The function visualizes clusters produced by a clustering model in 2D space, by \n",
    "    using t-SNE for dimensionality reduction. \n",
    "\n",
    "    Returns a 2D scatter plot of the data points colored by their predicted cluster labels. \n",
    "    '''\n",
    "    embedding = TSNE(n_components=2,\n",
    "        init=\"pca\",\n",
    "        max_iter=500,\n",
    "        n_iter_without_progress=150,\n",
    "        perplexity= 20,\n",
    "        random_state=0)   \n",
    "\n",
    "    data_2D = embedding.fit_transform(data)\n",
    "    \n",
    "    labels = model.labels_\n",
    "    cmap = plt.get_cmap('tab10', model.n_clusters)\n",
    "\n",
    "    unique_labels = set(labels)\n",
    "    fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "    for l in unique_labels: \n",
    "        cluster = data_2D[labels == l]\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], color = cmap(l), label = f\"Cluster {l}\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_product_data(dataframes_scaled : dict, num_clusters : dict, cluster_features : list ,vizualize = False): \n",
    "    '''\n",
    "    For each dataframe in the dictionary that contains the scaled data, and the features that we proceed with we fit a KMeans\n",
    "    model to each of the dataframes, with the number of clusters specified in the num_clusters dictionary. \n",
    "    '''\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for category in dataframes_scaled.keys(): \n",
    "        cluster_data = scaler.fit_transform(dataframes_scaled[category][cluster_features])\n",
    "        model = KMeans(n_clusters = num_clusters[category], init = \"k-means++\", random_state=42)\n",
    "        predicted = model.fit_predict(cluster_data)\n",
    "        \n",
    "        if vizualize: visualize_clusters(model, cluster_data, category) \n",
    "\n",
    "        dataframes_scaled[category][\"ProductCluster\"] = predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_revenue(customer_clustering_data : dict):\n",
    "    result = {}\n",
    "\n",
    "    for category in customer_clustering_data.keys():\n",
    "        data = customer_clustering_data[category]\n",
    "        cluster_revenues_customer = data.groupby([\"CustomerID\", \"ProductCluster\"])[\"TotalSales\"].sum()\n",
    "        cluster_revenues_customer = cluster_revenues_customer.unstack(fill_value = 0)\n",
    "        cluster_revenues_customer.columns = [f\"totalRevenue_PC_{col}\" for col in cluster_revenues_customer]\n",
    "        cluster_revenues_customer = cluster_revenues_customer.reset_index()\n",
    "\n",
    "        result[category] = pd.merge(data, cluster_revenues_customer, on = \"CustomerID\", how = \"left\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_customer_data(customer_clustering_data : dict, n_clusters : dict, cluster_features, vizualise = False):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for category in customer_clustering_data.keys():\n",
    "        cluster_data = scaler.fit_transform(customer_clustering_data[category][cluster_features])\n",
    "        model = KMeans(n_clusters=n_clusters[category], init = \"k-means++\", random_state=42)\n",
    "        predictions = model.fit_predict(cluster_data)\n",
    "\n",
    "        if vizualise: visualize_clusters(model, cluster_data, category)\n",
    "\n",
    "        customer_clustering_data[category][\"CustomerCluster\"] = predictions\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data: array of tuples with 'cluster_id' and 'margin'\n",
    "# bytt ut data med customercluster info\n",
    "\n",
    "def get_lower_upper_margin(data, customer_cluster): \n",
    "    # Extract 'margin' and 'cluster_id' arrays\n",
    "    margins = data['margin']\n",
    "    cluster_ids = data['cluster_id']\n",
    "\n",
    "    def percentile_rank(value, data_array):\n",
    "        \"\"\"Calculate the percentile rank of a value within an array.\"\"\"\n",
    "        sorted_array = np.sort(data_array)\n",
    "        count = np.searchsorted(sorted_array, value, side='left')\n",
    "        percentile = (count / len(data_array)) * 100\n",
    "        return percentile\n",
    "\n",
    "    # Get unique cluster IDs\n",
    "    unique_clusters = np.unique(cluster_ids)\n",
    "\n",
    "    # Dictionary to store results\n",
    "    cluster_percentiles = {}\n",
    "\n",
    "    # Calculate 40th and 75th percentiles for each cluster\n",
    "    for cluster in unique_clusters:\n",
    "        # Filter margins for the current cluster\n",
    "        cluster_margins = margins[cluster_ids == cluster]\n",
    "\n",
    "        # Calculate 40th and 75th percentiles within the cluster\n",
    "        percentile_40 = np.percentile(cluster_margins, 40)\n",
    "        percentile_75 = np.percentile(cluster_margins, 75)\n",
    "\n",
    "        # Calculate percentile ranks for the overall data\n",
    "        percentile_40_rank = percentile_rank(percentile_40, margins)\n",
    "        percentile_75_rank = percentile_rank(percentile_75, margins)\n",
    "\n",
    "        # Store the results\n",
    "        cluster_percentiles[cluster] = {\n",
    "            '40th_percentile_margin': percentile_40,\n",
    "            '40th_percentile_rank': percentile_40_rank,\n",
    "            '75th_percentile_margin': percentile_75,\n",
    "            '75th_percentile_rank': percentile_75_rank\n",
    "        }\n",
    "\n",
    "    lower_margin = cluster_percentiles[customer_cluster]['40th_percentile_margin']\n",
    "    upper_margin = cluster_percentiles[customer_cluster]['75th_percentile_margin']\n",
    "\n",
    "    return lower_margin, upper_margin\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price(margin, cost):\n",
    "    return cost/(1-margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#category = get_coefficient_variation(data, \"Category\") # We found that the it is best to split the data on category by looking at the coefficient of variation. \n",
    "#sub_category = get_coefficient_variation(data, \"SubCategory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Recommender Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_clustering_pipeline(ecommerce_df : pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    The function takes the original dataframe for US ECOMM RECORDS 2020 and first splits the dataframe\n",
    "    based on Category. Then appropriate feature engineering to get a dataframe ready for clustering using KMeans.\n",
    "\n",
    "    The KMeans algorithm is applied and (you select the number of appropriate clusters based on inertia and silhouette graphs)\n",
    "    and the identified cluster for each row is added as a feature. \n",
    "\n",
    "    This accquired dataframe is returned by this function and can be used as input for customer clustering. \n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    ecommerce_grouped = split_data(feature = \"Category\", data = ecommerce_df)\n",
    "    ecommerce_engineered = feature_engineering_preprocessing(ecommerce_grouped, scaler)\n",
    "    engineered_features = [\"TotalCost\",\t\"TotalSales\", \"NumberOfOrders\", \"MeanMargin\"]\n",
    "    num_clusters = {\"Technology\" : 5, \"Furniture\" : 5, \"Office Supplies\" : 5}\n",
    "    #find_optimal_k(ecommerce_engineered, engineered_features)\n",
    "    #num_clusters = input(\"Enter optimal number of clusters for each category according to the plot.\")\n",
    "    #num_clusters = {key.strip(): int(value) for key, value in (item.split(\":\") for item in num_clusters.split(\", \"))}\n",
    "\n",
    "    cluster_product_data(ecommerce_engineered, num_clusters, engineered_features)\n",
    "    \n",
    "    return ecommerce_engineered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_clustering_pipeline(customer_clustering_data : dict) -> dict:\n",
    "    customer_clustering_data = calculate_total_revenue(customer_clustering_data)\n",
    "    clustering_features = [\"totalRevenue_PC_0\",\t\"totalRevenue_PC_1\",\t\"totalRevenue_PC_2\",\t\"totalRevenue_PC_3\",\t\"totalRevenue_PC_4\"]\n",
    "    num_clusters = {\"Technology\" : 5, \"Furniture\" : 5, \"Office Supplies\" : 5}\n",
    "    cluster_customer_data(customer_clustering_data, num_clusters, clustering_features)\n",
    "\n",
    "    return customer_clustering_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommended_price(complete_data : dict, customer_id : str, product_id : str):\n",
    "    unsplit_complete_data = pd.concat(complete_data.values(), ignore_index=True)\n",
    "    category = unsplit_complete_data[unsplit_complete_data[\"ProductID\"] == product_id][\"Category\"].unique()[0]\n",
    "    product_cluster = unsplit_complete_data[unsplit_complete_data[\"ProductID\"] == product_id][\"ProductCluster\"].unique()[0]\n",
    "    print(product_cluster)\n",
    "\n",
    "    percentile_data = {}\n",
    "    for category in complete_data.keys():\n",
    "        data = complete_data[category]\n",
    "        data = data[data[\"ProductCluster\"] == product_cluster]\n",
    "        print(data)\n",
    "        data = data[[\"CustomerCluster\", \"Margin\"]]\n",
    "        data_array = np.array(list(data.itertuples(index = False, name = None)), \n",
    "                                 dtype=[(\"cluster_id\", int), (\"margin\", float)])\n",
    "        percentile_data[category] = data_array\n",
    "    \n",
    "    \n",
    "    data = complete_data[category]\n",
    "    cost = unsplit_complete_data[unsplit_complete_data[\"ProductID\"] == product_id][\"Cost\"].unique()[0]\n",
    "    \n",
    "    customer_cluster = data.loc[data[\"CustomerID\"] == customer_id, \"CustomerCluster\"].values[0]\n",
    "\n",
    "    margin_lower, margin_upper = get_lower_upper_margin(percentile_data[category], customer_cluster)\n",
    "    lower_price, upper_price = calculate_price(margin_lower, cost), calculate_price(margin_upper, cost)\n",
    "    recommended_price = calculate_price((margin_lower + margin_upper)/2, cost)\n",
    "\n",
    "    return lower_price, upper_price, recommended_price\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(customer_id, product_id):\n",
    "    filepath = 'CogitoXInfor-h24/ECOMMRecords2020.csv'\n",
    "    ecommerce_df = pd.read_csv(filepath)\n",
    "\n",
    "    ecommerce_df[\"Cost\"] = (ecommerce_df[\"Sales\"] - ecommerce_df[\"Profit\"])/ecommerce_df[\"Quantity\"] \n",
    "    ecommerce_df[\"Margin\"] = ecommerce_df[\"Profit\"]/ecommerce_df[\"Sales\"]\n",
    "\n",
    "    customer_clustering_data =  product_clustering_pipeline(ecommerce_df)\n",
    "    complete_data = customer_clustering_pipeline(customer_clustering_data)\n",
    "    print(complete_data[\"Technology\"].columns)\n",
    "    lower_price, upper_price, recommended_price = compute_recommended_price(complete_data, customer_id, product_id)\n",
    "\n",
    "\n",
    "    print(f\"For product {product_id} and the customer {customer_id} we recommend the following:\")\n",
    "    print(f\"Lower sales price: {lower_price:.2f}\")\n",
    "    print(f\"Upper sales price: {upper_price:.2f}\")\n",
    "    print(f\"Recommended sales price: {recommended_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ProductName', 'MeanMargin', 'OrderDate', 'RowID', 'OrderID',\n",
      "       'ShipMode', 'CustomerID', 'Segment', 'Country', 'City', 'State',\n",
      "       'PostalCode', 'Region', 'ProductID', 'Category', 'SubCategory', 'Sales',\n",
      "       'Quantity', 'Discount', 'Profit', 'Cost', 'Margin', 'NumberOfOrders',\n",
      "       'TotalSales', 'TotalCost', 'ProductCluster', 'totalRevenue_PC_0',\n",
      "       'totalRevenue_PC_1', 'totalRevenue_PC_2', 'totalRevenue_PC_3',\n",
      "       'totalRevenue_PC_4', 'CustomerCluster'],\n",
      "      dtype='object')\n",
      "0\n",
      "                                           ProductName  MeanMargin  \\\n",
      "3                             24-Hour Round Wall Clock      0.4300   \n",
      "4                             24-Hour Round Wall Clock      0.4300   \n",
      "11   3M Polarizing Task Lamp with Clamp Arm Light Gray      0.2600   \n",
      "12   3M Polarizing Task Lamp with Clamp Arm Light Gray      0.2600   \n",
      "13                          6 Cubicle Wall Clock Black     -0.1200   \n",
      "..                                                 ...         ...   \n",
      "681               Westinghouse Clip-On Gooseneck Lamps      0.1675   \n",
      "682               Westinghouse Clip-On Gooseneck Lamps      0.1675   \n",
      "683  Westinghouse Floor Lamp with Metal Mesh Shade ...      0.2300   \n",
      "684  Westinghouse Floor Lamp with Metal Mesh Shade ...      0.2300   \n",
      "685  Westinghouse Mesh Shade Clip-On Gooseneck Lamp...      0.2600   \n",
      "\n",
      "      OrderDate  RowID         OrderID        ShipMode CustomerID  \\\n",
      "3    2020-05-15   2632  US-2017-110604  Standard Class   JF-15295   \n",
      "4    2020-10-04   5582  CA-2017-163818  Standard Class   PS-18970   \n",
      "11   2020-04-08   5371  US-2017-136721  Standard Class   NH-18610   \n",
      "12   2020-11-19   2552  CA-2017-101042  Standard Class   AB-10105   \n",
      "13   2020-09-11   5270  CA-2017-144484        Same Day   CB-12025   \n",
      "..          ...    ...             ...             ...        ...   \n",
      "681  2020-03-10   1256  CA-2017-144638  Standard Class   MH-18115   \n",
      "682  2020-04-02   1337  US-2017-123281  Standard Class   JF-15190   \n",
      "683  2020-03-06   3736  CA-2017-168193    Second Class   RM-19750   \n",
      "684  2020-07-03   1304  US-2017-126179  Standard Class   CS-12460   \n",
      "685  2020-10-19    423  CA-2017-125388  Standard Class   MP-17965   \n",
      "\n",
      "         Segment        Country           City  ... NumberOfOrders  \\\n",
      "3       Consumer  United States        Seattle  ...              2   \n",
      "4    Home Office  United States        Clinton  ...              2   \n",
      "11     Corporate  United States       Oak Park  ...              2   \n",
      "12      Consumer  United States      Henderson  ...              2   \n",
      "13      Consumer  United States  San Francisco  ...              2   \n",
      "..           ...            ...            ...  ...            ...   \n",
      "681  Home Office  United States        Chester  ...              2   \n",
      "682     Consumer  United States    Los Angeles  ...              2   \n",
      "683     Consumer  United States  New York City  ...              2   \n",
      "684     Consumer  United States       Columbus  ...              2   \n",
      "685    Corporate  United States       Lawrence  ...              1   \n",
      "\n",
      "     TotalSales TotalCost ProductCluster totalRevenue_PC_0 totalRevenue_PC_1  \\\n",
      "3        59.940   22.7772              0            59.940           143.856   \n",
      "4        59.940   22.7772              0            59.940            57.564   \n",
      "11     1095.840  202.7304              0          1095.840           920.700   \n",
      "12     1095.840  202.7304              0          1095.840             0.000   \n",
      "13       42.068   10.3552              0            42.068             0.000   \n",
      "..          ...       ...            ...               ...               ...   \n",
      "681      31.806   12.3876              0            31.806           842.076   \n",
      "682      31.806   12.3876              0            31.806             0.000   \n",
      "683      95.960   36.9446              0            95.960             0.000   \n",
      "684      95.960   36.9446              0            95.960             0.000   \n",
      "685      56.560   10.4636              0            56.560             0.000   \n",
      "\n",
      "     totalRevenue_PC_2  totalRevenue_PC_3  totalRevenue_PC_4  CustomerCluster  \n",
      "3              2292.46               0.00                0.0                1  \n",
      "4                 0.00               0.00                0.0                1  \n",
      "11                0.00               0.00                0.0                3  \n",
      "12                0.00               0.00                0.0                1  \n",
      "13                0.00               0.00                0.0                1  \n",
      "..                 ...                ...                ...              ...  \n",
      "681               0.00             982.35                0.0                3  \n",
      "682               0.00               0.00                0.0                1  \n",
      "683               0.00               0.00                0.0                1  \n",
      "684               0.00               0.00                0.0                1  \n",
      "685               0.00               0.00                0.0                1  \n",
      "\n",
      "[188 rows x 32 columns]\n",
      "                                            ProductName  MeanMargin  \\\n",
      "170                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "171                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "172                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "173                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "174                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "175                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "176                  Adjustable Depth Letter/Legal Cart    0.239286   \n",
      "752         Fellowes PB300 Plastic Comb Binding Machine    0.370625   \n",
      "753         Fellowes PB300 Plastic Comb Binding Machine    0.370625   \n",
      "754         Fellowes PB300 Plastic Comb Binding Machine    0.370625   \n",
      "755         Fellowes PB300 Plastic Comb Binding Machine    0.370625   \n",
      "756   Fellowes PB500 Electric Punch Plastic Comb Bin...   -0.875000   \n",
      "757   Fellowes PB500 Electric Punch Plastic Comb Bin...   -0.875000   \n",
      "758   Fellowes PB500 Electric Punch Plastic Comb Bin...   -0.875000   \n",
      "798           GBC DocuBind 300 Electric Binding Machine    0.144167   \n",
      "799           GBC DocuBind 300 Electric Binding Machine    0.144167   \n",
      "800           GBC DocuBind 300 Electric Binding Machine    0.144167   \n",
      "801           GBC DocuBind 300 Electric Binding Machine    0.144167   \n",
      "804           GBC DocuBind P400 Electric Binding System   -0.170000   \n",
      "805           GBC DocuBind P400 Electric Binding System   -0.170000   \n",
      "813          GBC DocuBind TL300 Electric Binding System   -0.051167   \n",
      "814          GBC DocuBind TL300 Electric Binding System   -0.051167   \n",
      "815          GBC DocuBind TL300 Electric Binding System   -0.051167   \n",
      "816          GBC DocuBind TL300 Electric Binding System   -0.051167   \n",
      "817          GBC DocuBind TL300 Electric Binding System   -0.051167   \n",
      "1057  Martin Yale Chadless Opener Electric Letter Op...   -0.102500   \n",
      "1058  Martin Yale Chadless Opener Electric Letter Op...   -0.102500   \n",
      "1059  Martin Yale Chadless Opener Electric Letter Op...   -0.102500   \n",
      "1060  Martin Yale Chadless Opener Electric Letter Op...   -0.102500   \n",
      "1545                        Tennsco Double-Tier Lockers    0.021250   \n",
      "1546                        Tennsco Double-Tier Lockers    0.021250   \n",
      "1547                        Tennsco Double-Tier Lockers    0.021250   \n",
      "1548                        Tennsco Double-Tier Lockers    0.021250   \n",
      "1555                        Tennsco Single-Tier Lockers   -0.009375   \n",
      "1556                        Tennsco Single-Tier Lockers   -0.009375   \n",
      "1557                        Tennsco Single-Tier Lockers   -0.009375   \n",
      "1558                        Tennsco Single-Tier Lockers   -0.009375   \n",
      "\n",
      "       OrderDate  RowID         OrderID        ShipMode CustomerID  \\\n",
      "170   2020-03-21   5346  CA-2017-108539    Second Class   SC-20725   \n",
      "171   2020-10-06   9872  CA-2017-146269        Same Day   MH-17455   \n",
      "172   2020-10-07   3386  CA-2017-148404  Standard Class   Dp-13240   \n",
      "173   2020-11-09   3184  CA-2017-152912    Second Class   BM-11650   \n",
      "174   2020-11-09   3185  CA-2017-152912    Second Class   BM-11650   \n",
      "175   2020-12-17   4342  CA-2017-130309  Standard Class   GB-14575   \n",
      "176   2020-12-28    271  CA-2017-163979    Second Class   KH-16690   \n",
      "752   2020-08-18   6210  CA-2017-119809  Standard Class   YS-21880   \n",
      "753   2020-08-19   4415  CA-2017-132339  Standard Class   JB-15400   \n",
      "754   2020-09-26   7531  US-2017-106145        Same Day   RA-19885   \n",
      "755   2020-11-10   3218  CA-2017-104640  Standard Class   FH-14275   \n",
      "756   2020-09-10   2847  CA-2017-152093  Standard Class   SN-20560   \n",
      "757   2020-10-13   8859  CA-2017-135909  Standard Class   JW-15220   \n",
      "758   2020-11-19   5311  CA-2017-131254     First Class   NC-18415   \n",
      "798   2020-08-27   3754  CA-2017-161956    Second Class   DR-12880   \n",
      "799   2020-09-02   9274  CA-2017-146983  Standard Class   AH-10210   \n",
      "800   2020-09-07   3567  CA-2017-103877  Standard Class   RD-19660   \n",
      "801   2020-12-02   4498  CA-2017-155075  Standard Class   GW-14605   \n",
      "804   2020-01-16   6521  CA-2017-138289    Second Class   AR-10540   \n",
      "805   2020-10-19   2929  US-2017-120390  Standard Class   TH-21550   \n",
      "813   2020-04-08   2849  CA-2017-157854  Standard Class   DM-13345   \n",
      "814   2020-08-13   7899  CA-2017-128363  Standard Class   DC-12850   \n",
      "815   2020-08-21   9271  US-2017-102183  Standard Class   PK-19075   \n",
      "816   2020-09-16   3460  CA-2017-136448     First Class   AS-10090   \n",
      "817   2020-10-03   1897  CA-2017-141789     First Class   AC-10450   \n",
      "1057  2020-01-13   6621  US-2017-167402    Second Class   CP-12085   \n",
      "1058  2020-03-13   4459  CA-2017-105851  Standard Class   SE-20110   \n",
      "1059  2020-09-22   1804  CA-2017-158379    Second Class   JA-15970   \n",
      "1060  2020-12-19   3589  CA-2017-158729     First Class   AC-10450   \n",
      "1545  2020-04-22   8280  CA-2017-146192  Standard Class   BD-11725   \n",
      "1546  2020-10-02   4137  CA-2017-105914  Standard Class   PV-18985   \n",
      "1547  2020-11-12   1502  CA-2017-130386  Standard Class   NG-18430   \n",
      "1548  2020-11-13   3784  CA-2017-165204    Second Class   MN-17935   \n",
      "1555  2020-01-22    519  CA-2017-127432  Standard Class   AD-10180   \n",
      "1556  2020-08-15   2535  US-2017-117723    Second Class   DL-13495   \n",
      "1557  2020-12-21   4944  CA-2017-106782  Standard Class   LP-17095   \n",
      "1558  2020-12-26   5475  CA-2017-121741        Same Day   YC-21895   \n",
      "\n",
      "          Segment        Country           City  ... NumberOfOrders  \\\n",
      "170      Consumer  United States    Los Angeles  ...              7   \n",
      "171      Consumer  United States        Chicago  ...              7   \n",
      "172   Home Office  United States      Charlotte  ...              7   \n",
      "173     Corporate  United States       Columbia  ...              7   \n",
      "174     Corporate  United States       Columbia  ...              7   \n",
      "175      Consumer  United States      Arlington  ...              7   \n",
      "176     Corporate  United States  San Francisco  ...              7   \n",
      "752     Corporate  United States        Seattle  ...              4   \n",
      "753     Corporate  United States       Lawrence  ...              4   \n",
      "754     Corporate  United States  San Francisco  ...              4   \n",
      "755     Corporate  United States  New York City  ...              4   \n",
      "756   Home Office  United States        Chicago  ...              3   \n",
      "757     Corporate  United States     Sacramento  ...              3   \n",
      "758      Consumer  United States        Houston  ...              3   \n",
      "798     Corporate  United States      Inglewood  ...              4   \n",
      "799      Consumer  United States      Henderson  ...              4   \n",
      "800   Home Office  United States   Independence  ...              4   \n",
      "801      Consumer  United States   Philadelphia  ...              4   \n",
      "804      Consumer  United States        Jackson  ...              2   \n",
      "805   Home Office  United States     Burlington  ...              2   \n",
      "813     Corporate  United States        Roswell  ...              5   \n",
      "814      Consumer  United States        Memphis  ...              5   \n",
      "815      Consumer  United States  New York City  ...              5   \n",
      "816      Consumer  United States   Philadelphia  ...              5   \n",
      "817      Consumer  United States    Minneapolis  ...              5   \n",
      "1057    Corporate  United States    Springfield  ...              4   \n",
      "1058     Consumer  United States         Denver  ...              4   \n",
      "1059     Consumer  United States   Philadelphia  ...              4   \n",
      "1060     Consumer  United States      Lafayette  ...              4   \n",
      "1545     Consumer  United States       Columbus  ...              4   \n",
      "1546  Home Office  United States    Los Angeles  ...              4   \n",
      "1547     Consumer  United States         Austin  ...              4   \n",
      "1548     Consumer  United States        Memphis  ...              4   \n",
      "1555  Home Office  United States    Great Falls  ...              4   \n",
      "1556    Corporate  United States   Philadelphia  ...              4   \n",
      "1557     Consumer  United States      Lafayette  ...              4   \n",
      "1558    Corporate  United States        Fremont  ...              4   \n",
      "\n",
      "      TotalSales  TotalCost ProductCluster totalRevenue_PC_0  \\\n",
      "170     5044.588   901.8562              0          5044.588   \n",
      "171     5044.588   901.8562              0          5044.588   \n",
      "172     5044.588   901.8562              0          5044.588   \n",
      "173     5044.588   901.8562              0         10089.176   \n",
      "174     5044.588   901.8562              0         10089.176   \n",
      "175     5044.588   901.8562              0          5044.588   \n",
      "176     5044.588   901.8562              0          5044.588   \n",
      "752     5043.870   822.5388              0          5043.870   \n",
      "753     5043.870   822.5388              0          5043.870   \n",
      "754     5043.870   822.5388              0          5043.870   \n",
      "755     5043.870   822.5388              0          5043.870   \n",
      "756     7371.742  1906.4850              0          7371.742   \n",
      "757     7371.742  1906.4850              0          7371.742   \n",
      "758     7371.742  1906.4850              0          7371.742   \n",
      "798     4628.624  1094.0384              0          4628.624   \n",
      "799     4628.624  1094.0384              0          4628.624   \n",
      "800     4628.624  1094.0384              0          4628.624   \n",
      "801     4628.624  1094.0384              0          4628.624   \n",
      "804     7077.148  1469.8692              0          7077.148   \n",
      "805     7077.148  1469.8692              0          7077.148   \n",
      "813    10943.278  2377.0235              0         10943.278   \n",
      "814    10943.278  2377.0235              0         10943.278   \n",
      "815    10943.278  2377.0235              0         10943.278   \n",
      "816    10943.278  2377.0235              0         10943.278   \n",
      "817    10943.278  2377.0235              0         22769.180   \n",
      "1057   11825.902  3264.6152              0         11825.902   \n",
      "1058   11825.902  3264.6152              0         11825.902   \n",
      "1059   11825.902  3264.6152              0         11825.902   \n",
      "1060   11825.902  3264.6152              0         22769.180   \n",
      "1545    3510.312   783.0696              0          3510.312   \n",
      "1546    3510.312   783.0696              0          3510.312   \n",
      "1547    3510.312   783.0696              0          3510.312   \n",
      "1548    3510.312   783.0696              0          3510.312   \n",
      "1555    4053.672  1426.2920              0          4053.672   \n",
      "1556    4053.672  1426.2920              0          4053.672   \n",
      "1557    4053.672  1426.2920              0          4053.672   \n",
      "1558    4053.672  1426.2920              0          4053.672   \n",
      "\n",
      "     totalRevenue_PC_1  totalRevenue_PC_2  totalRevenue_PC_3  \\\n",
      "170             32.256              0.000              0.000   \n",
      "171            172.510              0.000              0.000   \n",
      "172            273.098              0.000            117.936   \n",
      "173           1582.752              0.000           3438.371   \n",
      "174           1582.752              0.000           3438.371   \n",
      "175            369.468              0.000              0.000   \n",
      "176              0.000              0.000              0.000   \n",
      "752             69.000              0.000              0.000   \n",
      "753              0.000             29.410            176.766   \n",
      "754            142.740              0.000              0.000   \n",
      "755            117.034              0.000            311.520   \n",
      "756              0.000              0.000              0.000   \n",
      "757              0.000              0.000              0.000   \n",
      "758             13.040              0.000              0.000   \n",
      "798            229.202              0.000              0.000   \n",
      "799            183.968              0.000           1010.824   \n",
      "800              0.000              0.000              0.000   \n",
      "801              0.000              0.000              0.000   \n",
      "804            103.750              0.000              0.000   \n",
      "805              4.540              0.000           1130.226   \n",
      "813              0.000              0.000            433.550   \n",
      "814            203.578              0.000              0.000   \n",
      "815            126.000              0.000              0.000   \n",
      "816            160.280             24.108              0.000   \n",
      "817              0.000              0.000              0.000   \n",
      "1057            64.800              0.000             24.840   \n",
      "1058             0.000              0.000           1517.192   \n",
      "1059            90.442             23.760              0.000   \n",
      "1060             0.000              0.000              0.000   \n",
      "1545             3.890              0.000              0.000   \n",
      "1546             0.000              0.000              0.000   \n",
      "1547           255.168              0.000              0.000   \n",
      "1548            36.992              0.000              0.000   \n",
      "1555            26.312              0.000              0.000   \n",
      "1556           398.480              0.000              0.000   \n",
      "1557           144.760              0.000            140.816   \n",
      "1558             0.000              0.000              0.000   \n",
      "\n",
      "      totalRevenue_PC_4  CustomerCluster  \n",
      "170               0.000                4  \n",
      "171               0.000                4  \n",
      "172             880.389                4  \n",
      "173             102.980                0  \n",
      "174             102.980                0  \n",
      "175               0.000                4  \n",
      "176               0.000                4  \n",
      "752            5436.808                3  \n",
      "753             505.632                4  \n",
      "754               0.000                4  \n",
      "755             762.996                4  \n",
      "756              83.720                4  \n",
      "757             101.324                4  \n",
      "758               0.000                4  \n",
      "798            1561.008                3  \n",
      "799            2007.240                3  \n",
      "800               0.000                4  \n",
      "801               0.000                4  \n",
      "804               0.000                4  \n",
      "805              68.456                0  \n",
      "813             751.586                4  \n",
      "814             754.968                4  \n",
      "815             180.233                4  \n",
      "816             103.950                4  \n",
      "817               0.000                4  \n",
      "1057            815.484                4  \n",
      "1058             87.792                0  \n",
      "1059            311.559                4  \n",
      "1060              0.000                4  \n",
      "1545            144.900                4  \n",
      "1546            184.998                4  \n",
      "1547              0.000                4  \n",
      "1548             25.228                4  \n",
      "1555            981.774                4  \n",
      "1556            427.420                4  \n",
      "1557             85.140                4  \n",
      "1558              0.000                4  \n",
      "\n",
      "[37 rows x 32 columns]\n",
      "                                  ProductName  MeanMargin   OrderDate  RowID  \\\n",
      "16        Aastra 6757i CT Wireless VoIP phone    0.082917  2020-05-18   4715   \n",
      "17        Aastra 6757i CT Wireless VoIP phone    0.082917  2020-08-31   4991   \n",
      "18        Aastra 6757i CT Wireless VoIP phone    0.082917  2020-09-08   3127   \n",
      "19        Aastra 6757i CT Wireless VoIP phone    0.082917  2020-11-10   3219   \n",
      "102                     Digium D40 VoIP phone    0.082917  2020-03-06   6515   \n",
      "..                                        ...         ...         ...    ...   \n",
      "607  Wilson Electronics DB Pro Signal Booster   -0.064583  2020-11-02   5506   \n",
      "612  iHome FM Clock Radio with Lightning Dock   -0.015625  2020-01-14   4912   \n",
      "613  iHome FM Clock Radio with Lightning Dock   -0.015625  2020-02-18   5388   \n",
      "614  iHome FM Clock Radio with Lightning Dock   -0.015625  2020-06-10    487   \n",
      "615  iHome FM Clock Radio with Lightning Dock   -0.015625  2020-12-08   8740   \n",
      "\n",
      "            OrderID        ShipMode CustomerID      Segment        Country  \\\n",
      "16   CA-2017-121643     First Class   AB-10105     Consumer  United States   \n",
      "17   CA-2017-107321  Standard Class   AW-10930  Home Office  United States   \n",
      "18   CA-2017-149895  Standard Class   EB-14110     Consumer  United States   \n",
      "19   CA-2017-104640  Standard Class   FH-14275    Corporate  United States   \n",
      "102  CA-2017-167640  Standard Class   FC-14245  Home Office  United States   \n",
      "..              ...             ...        ...          ...            ...   \n",
      "607  CA-2017-127782  Standard Class   TH-21115    Corporate  United States   \n",
      "612  CA-2017-127306  Standard Class   BH-11710     Consumer  United States   \n",
      "613  CA-2017-164707    Second Class   CV-12805    Corporate  United States   \n",
      "614  CA-2017-140963     First Class   MT-18070  Home Office  United States   \n",
      "615  US-2017-131961     First Class   MJ-17740     Consumer  United States   \n",
      "\n",
      "              City  ... NumberOfOrders  TotalSales  TotalCost ProductCluster  \\\n",
      "16        Portland  ...              5    3655.478   943.0424              0   \n",
      "17   San Francisco  ...              5    3655.478   943.0424              0   \n",
      "18    Philadelphia  ...              5    3655.478   943.0424              0   \n",
      "19   New York City  ...              5    3655.478   943.0424              0   \n",
      "102  San Francisco  ...              4     877.132   366.3316              0   \n",
      "..             ...  ...            ...         ...        ...            ...   \n",
      "607   Philadelphia  ...              4    3293.600  1045.3600              0   \n",
      "612   Johnson City  ...              4     643.908   209.9700              0   \n",
      "613    Los Angeles  ...              4     643.908   209.9700              0   \n",
      "614    Los Angeles  ...              4     643.908   209.9700              0   \n",
      "615   Philadelphia  ...              4     643.908   209.9700              0   \n",
      "\n",
      "    totalRevenue_PC_0 totalRevenue_PC_1  totalRevenue_PC_2  totalRevenue_PC_3  \\\n",
      "16           3828.014             0.000           3480.282                0.0   \n",
      "17           3655.478             0.000              0.000                0.0   \n",
      "18           3655.478             0.000              0.000                0.0   \n",
      "19           3655.478           597.132             77.922                0.0   \n",
      "102           877.132           158.376              0.000                0.0   \n",
      "..                ...               ...                ...                ...   \n",
      "607          3293.600             0.000              0.000                0.0   \n",
      "612           643.908            36.048            119.600                0.0   \n",
      "613           643.908             0.000            226.362                0.0   \n",
      "614           643.908             0.000              0.000                0.0   \n",
      "615           643.908             0.000              0.000                0.0   \n",
      "\n",
      "     totalRevenue_PC_4  CustomerCluster  \n",
      "16                 0.0                0  \n",
      "17                 0.0                0  \n",
      "18                 0.0                0  \n",
      "19                 0.0                0  \n",
      "102                0.0                1  \n",
      "..                 ...              ...  \n",
      "607                0.0                0  \n",
      "612                0.0                1  \n",
      "613                0.0                1  \n",
      "614                0.0                1  \n",
      "615                0.0                1  \n",
      "\n",
      "[171 rows x 32 columns]\n",
      "For product FUR-FU-10001215 and the customer JF-15295 we recommend the following:\n",
      "Lower sales price: 30.77\n",
      "Upper sales price: 37.65\n",
      "Recommended sales price: 33.86\n"
     ]
    }
   ],
   "source": [
    "pipeline(\"JF-15295\", \"FUR-FU-10001215\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
