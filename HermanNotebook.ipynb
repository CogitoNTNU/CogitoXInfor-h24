{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Recommender by Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to predict recommended margin range for when a customer wants to by a new product by using unsupervised learning. \n",
    "More specifically we will first cluster products and then customers. After the clustering is performed we can calcuate the upper and lower bound for recommended margin with the following formula: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the necessary libraries, and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "This section contains a variety of important functions used throughout the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficient_variation(data, feature): \n",
    "    '''\n",
    "    The function calculates the coefficient of variation (CV) of the \"Margin\" for each category in the specified feature.\n",
    "\n",
    "    Returns a sorted DataFrame with the mean, standard deviation, and CV for each category.\n",
    "    '''\n",
    "    \n",
    "    grouped_data = data.groupby(feature)\n",
    "    mean, std = grouped_data[\"Margin\"].mean(), grouped_data[\"Margin\"].std()\n",
    "    df = pd.concat([mean, std], axis = 1).reset_index()\n",
    "    df.columns = [feature, \"Mean\", \"Std\"]\n",
    "    df[\"CoefficientOfVariation\"] = (df[\"Std\"]/df[\"Mean\"])**2\n",
    "\n",
    "    return df.sort_values(by = \"CoefficientOfVariation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_data(feature, data):\n",
    "    '''\n",
    "    Splits a DataFrame into multiple DataFrames based on the unique values of a specified categorical feature.\n",
    "\n",
    "    Parameters:\n",
    "    - feature: The column name to split the DataFrame by.\n",
    "    - data: The DataFrame to split.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where keys are unique feature values and values are the corresponding DataFrames.\n",
    "    '''\n",
    "\n",
    "    categories = list(data[feature].unique())\n",
    "    dataframes = {}\n",
    "\n",
    "    for c in categories: \n",
    "        df = data[data[feature] == c]\n",
    "        dataframes[c] = df\n",
    "        \n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "'''\n",
    "This cell contains all functions that are used to create new features used by the KMeans\n",
    "algorithms to group products in clusters. \n",
    "More functions are needed to create more features to achieve better clustering results. \n",
    "'''\n",
    "\n",
    "def add_average_margin_feature(dataframe):\n",
    "    means = dataframe.groupby(\"ProductName\")[\"Margin\"].mean().reset_index()\n",
    "    means.columns = [\"ProductName\", \"MeanMargin\"]\n",
    "    new_df = pd.merge(means, dataframe, on = \"ProductName\")\n",
    "    return new_df \n",
    "    \n",
    "\n",
    "def add_average_cost_per_unit_feature(dataframe):\n",
    "    dataframe.loc[:, \"CostPerUnit\"] = dataframe[\"Cost\"]/dataframe[\"Quantity\"]\n",
    "    means = dataframe.groupby(\"ProductName\")[\"CostPerUnit\"].mean().reset_index()\n",
    "    means.columns = [\"ProductName\", \"MeanCostPerUnit\"]\n",
    "    new_df = pd.merge(means, dataframe, on = \"ProductName\")\n",
    "    return new_df \n",
    "\n",
    "def add_average_sales_in_past_per_product(n_months, dataframe):\n",
    "    dataframe[\"OrderDate\"] = pd.to_datetime(dataframe[\"OrderDate\"])\n",
    "    last_date = dataframe[\"OrderDate\"].max()\n",
    "    n_months_ago = last_date - pd.DateOffset(months = n_months)\n",
    "    filtered_data = dataframe[dataframe[\"OrderDate\"] >= n_months_ago]\n",
    "    meanSales = filtered_data.groupby(\"ProductName\")[\"Sales\"].mean().reset_index()\n",
    "    meanSales.columns = [\"ProductName\", \"AverageSalesPastMonths\"]\n",
    "    new_df = pd.merge(meanSales, dataframe, on = \"ProductName\")\n",
    "    return new_df\n",
    "\n",
    "def number_of_orders_per_product(dataframe): \n",
    "    product_order_counts = dataframe.groupby(\"ProductID\")[\"OrderID\"].count().reset_index()\n",
    "    product_order_counts = product_order_counts.rename(columns = {\"OrderID\" : \"NumberOfOrders\"})\n",
    "    dataframe = dataframe.merge(product_order_counts, on = \"ProductID\", how = \"left\")\n",
    "    return dataframe\n",
    "    \n",
    "def add_total_sales_for_product(dataframe): \n",
    "    product_sales = dataframe.groupby(\"ProductID\")[\"Sales\"].sum().reset_index()\n",
    "    product_sales.columns = [\"ProductID\", \"TotalSales\"]\n",
    "    new_df = pd.merge(dataframe, product_sales, on = \"ProductID\", how = \"left\")\n",
    "    return new_df\n",
    "\n",
    "def add_total_cost_for_product(dataframe): \n",
    "    product_cost = dataframe.groupby(\"ProductID\")[\"Cost\"].sum().reset_index()\n",
    "    product_cost.columns = [\"ProductID\", \"TotalCost\"]\n",
    "    new_df = pd.merge(dataframe, product_cost, on = \"ProductID\", how = \"left\")\n",
    "    return new_df\n",
    "\n",
    "    \n",
    "\n",
    "# Group products based on sales. Use the TotalRevenue. \n",
    "# Start by picking two features and see if there is any clustering. For example revenue and avg margin, or margin and number of orders. \n",
    "# Do not use average but sum. Also try changing the scaling method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>RowID</th>\n",
       "      <th>OrderID</th>\n",
       "      <th>ShipMode</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>...</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1341</td>\n",
       "      <td>CA-2017-113481</td>\n",
       "      <td>First Class</td>\n",
       "      <td>AS-10045</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28540</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-MA-10002178</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Machines</td>\n",
       "      <td>Cisco CP-7937G Unified IP Conference Station P...</td>\n",
       "      <td>695.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-27.828</td>\n",
       "      <td>361.764</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OrderDate  RowID         OrderID     ShipMode CustomerID    Segment  \\\n",
       "10  2020-01-02   1341  CA-2017-113481  First Class   AS-10045  Corporate   \n",
       "\n",
       "          Country          City           State  PostalCode  ...  \\\n",
       "10  United States  Jacksonville  North Carolina       28540  ...   \n",
       "\n",
       "          ProductID    Category SubCategory  \\\n",
       "10  TEC-MA-10002178  Technology    Machines   \n",
       "\n",
       "                                          ProductName  Sales  Quantity  \\\n",
       "10  Cisco CP-7937G Unified IP Conference Station P...  695.7         2   \n",
       "\n",
       "    Discount  Profit     Cost  Margin  \n",
       "10       0.5 -27.828  361.764   -0.04  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'ECOMMRecords2020.csv'\n",
    "ecommerce_df = pd.read_csv(filepath)\n",
    "\n",
    "ecommerce_df[\"Cost\"] = (ecommerce_df[\"Sales\"] - ecommerce_df[\"Profit\"])/ecommerce_df[\"Quantity\"] \n",
    "ecommerce_df[\"Margin\"] = ecommerce_df[\"Profit\"]/ecommerce_df[\"Sales\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_preprocessing(dataframes: dict, scaler) -> dict:\n",
    "    '''\n",
    "    The function applies multiple feature engineering steps to each dataframe in the input dictionary. \n",
    "\n",
    "    It then returns a dictionary with the engineered dataframes. \n",
    "    '''\n",
    "    result = {}\n",
    "    for category in dataframes.keys():\n",
    "        df = dataframes[category]\n",
    "\n",
    "        df = number_of_orders_per_product(df)\n",
    "        df = add_total_sales_for_product(df)\n",
    "        df = add_total_cost_for_product(df)\n",
    "        df = add_average_margin_feature(df)\n",
    "\n",
    "        result[category] = df\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(dataframes : dict, engineered_features : list): \n",
    "    '''\n",
    "    Identifies the optimal number of clusters for KMeans using the elbow method and plotting the silhouette score. \n",
    "\n",
    "    Returns a plot of the interia score and silhouette score versus number of clusters for each of the dataframes \n",
    "    in the input dictionary. \n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    ks = [x for x in range(2, 11)]\n",
    "    inertia_scores = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for category in dataframes.keys(): \n",
    "        data = scaler.fit_transform(dataframes[category][engineered_features])\n",
    "        inertia = []\n",
    "        silhouette = []\n",
    "        for k in ks: \n",
    "            model = KMeans(n_clusters = k, random_state = 42, init = 'k-means++')\n",
    "            predicted = model.fit_predict(data)\n",
    "            inertia.append(model.inertia_)\n",
    "            silhouette.append(silhouette_score(data, predicted))\n",
    "\n",
    "        inertia_scores.append(inertia)\n",
    "        silhouette_scores.append(silhouette)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = len(dataframes.keys()), ncols = 2, figsize = (15, 14), squeeze=False)\n",
    "    for i in range(len(dataframes.keys())): \n",
    "        ax[i, 0].plot(ks, inertia_scores[i])\n",
    "        ax[i, 1].plot(ks, silhouette_scores[i])\n",
    "        ax[i, 0].set_xlabel(\"Number of Clusters\")\n",
    "        ax[i, 0].set_ylabel(\"Inertia\")\n",
    "        ax[i, 1].set_xlabel(\"Number of Clusters\")\n",
    "        ax[i, 1].set_ylabel(\"Silhouette Score\")\n",
    "        fig.text(0.5, 0.95 - (i*0.3), f\"{list(dataframes.keys())[i]}\", ha = 'center', fontsize = 14, weight = \"bold\")\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(model, data, title):\n",
    "    '''\n",
    "    The function visualizes clusters produced by a clustering model in 2D space, by \n",
    "    using t-SNE for dimensionality reduction. \n",
    "\n",
    "    Returns a 2D scatter plot of the data points colored by their predicted cluster labels. \n",
    "    '''\n",
    "    embedding = TSNE(n_components=2,\n",
    "        init=\"pca\",\n",
    "        max_iter=500,\n",
    "        n_iter_without_progress=150,\n",
    "        perplexity= 20,\n",
    "        random_state=0)   \n",
    "\n",
    "    data_2D = embedding.fit_transform(data)\n",
    "    \n",
    "    labels = model.labels_\n",
    "    cmap = plt.get_cmap('tab10', model.n_clusters)\n",
    "\n",
    "    unique_labels = set(labels)\n",
    "    fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "    for l in unique_labels: \n",
    "        cluster = data_2D[labels == l]\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], color = cmap(l), label = f\"Cluster {l}\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_product_data(dataframes_scaled : dict, num_clusters : dict, cluster_features : list ,vizualize = False): \n",
    "    '''\n",
    "    For each dataframe in the dictionary that contains the scaled data, and the features that we proceed with we fit a KMeans\n",
    "    model to each of the dataframes, with the number of clusters specified in the num_clusters dictionary. \n",
    "    '''\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for category in dataframes_scaled.keys(): \n",
    "        cluster_data = scaler.fit_transform(dataframes_scaled[category][cluster_features])\n",
    "        model = KMeans(n_clusters = num_clusters[category], init = \"k-means++\", random_state=42)\n",
    "        predicted = model.fit_predict(cluster_data)\n",
    "        \n",
    "        if vizualize: visualize_clusters(model, cluster_data, category) \n",
    "\n",
    "        dataframes_scaled[category][\"ProductCluster\"] = predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_revenue(customer_clustering_data : dict):\n",
    "    result = {}\n",
    "\n",
    "    for category in customer_clustering_data.keys():\n",
    "        data = customer_clustering_data[category]\n",
    "        cluster_revenues_customer = data.groupby([\"CustomerID\", \"ProductCluster\"])[\"TotalSales\"].sum()\n",
    "        cluster_revenues_customer = cluster_revenues_customer.unstack(fill_value = 0)\n",
    "        cluster_revenues_customer.columns = [f\"totalRevenue_PC_{col}\" for col in cluster_revenues_customer]\n",
    "        cluster_revenues_customer = cluster_revenues_customer.reset_index()\n",
    "\n",
    "        result[category] = pd.merge(data, cluster_revenues_customer, on = \"CustomerID\", how = \"left\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_customer_data(customer_clustering_data : dict, n_clusters : dict, cluster_features, vizualise = False):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for category in customer_clustering_data.keys():\n",
    "        cluster_data = scaler.fit_transform(customer_clustering_data[category][cluster_features])\n",
    "        model = KMeans(n_clusters=n_clusters[category], init = \"k-means++\", random_state=42)\n",
    "        predictions = model.fit_predict(cluster_data)\n",
    "\n",
    "        if vizualise: visualize_clusters(model, cluster_data, category)\n",
    "\n",
    "        customer_clustering_data[category][\"CustomerCluster\"] = predictions\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data: array of tuples with 'cluster_id' and 'margin'\n",
    "# bytt ut data med customercluster info\n",
    "\n",
    "def get_lower_upper_margin(data, customer_cluster): \n",
    "    # Extract 'margin' and 'cluster_id' arrays\n",
    "    margins = data['margin']\n",
    "    cluster_ids = data['cluster_id']\n",
    "\n",
    "    def percentile_rank(value, data_array):\n",
    "        \"\"\"Calculate the percentile rank of a value within an array.\"\"\"\n",
    "        sorted_array = np.sort(data_array)\n",
    "        count = np.searchsorted(sorted_array, value, side='left')\n",
    "        percentile = (count / len(data_array)) * 100\n",
    "        return percentile\n",
    "\n",
    "    # Get unique cluster IDs\n",
    "    unique_clusters = np.unique(cluster_ids)\n",
    "\n",
    "    # Dictionary to store results\n",
    "    cluster_percentiles = {}\n",
    "\n",
    "    # Calculate 40th and 75th percentiles for each cluster\n",
    "    for cluster in unique_clusters:\n",
    "        # Filter margins for the current cluster\n",
    "        cluster_margins = margins[cluster_ids == cluster]\n",
    "\n",
    "        # Calculate 40th and 75th percentiles within the cluster\n",
    "        percentile_40 = np.percentile(cluster_margins, 40)\n",
    "        percentile_75 = np.percentile(cluster_margins, 75)\n",
    "\n",
    "        # Calculate percentile ranks for the overall data\n",
    "        percentile_40_rank = percentile_rank(percentile_40, margins)\n",
    "        percentile_75_rank = percentile_rank(percentile_75, margins)\n",
    "\n",
    "        # Store the results\n",
    "        cluster_percentiles[cluster] = {\n",
    "            '40th_percentile_margin': percentile_40,\n",
    "            '40th_percentile_rank': percentile_40_rank,\n",
    "            '75th_percentile_margin': percentile_75,\n",
    "            '75th_percentile_rank': percentile_75_rank\n",
    "        }\n",
    "\n",
    "    lower_margin = cluster_percentiles[customer_cluster]['40th_percentile_margin']\n",
    "    upper_margin = cluster_percentiles[customer_cluster]['75th_percentile_margin']\n",
    "\n",
    "    return lower_margin, upper_margin\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price(margin, cost):\n",
    "    return cost/(1-margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category = get_coefficient_variation(data, \"Category\") # We found that the it is best to split the data on category by looking at the coefficient of variation. \n",
    "sub_category = get_coefficient_variation(data, \"SubCategory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Recommender Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_clustering_pipeline(ecommerce_df : pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    The function takes the original dataframe for US ECOMM RECORDS 2020 and first splits the dataframe\n",
    "    based on Category. Then appropriate feature engineering to get a dataframe ready for clustering using KMeans.\n",
    "\n",
    "    The KMeans algorithm is applied and (you select the number of appropriate clusters based on inertia and silhouette graphs)\n",
    "    and the identified cluster for each row is added as a feature. \n",
    "\n",
    "    This accquired dataframe is returned by this function and can be used as input for customer clustering. \n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    ecommerce_grouped = split_data(feature = \"Category\", data = ecommerce_df)\n",
    "    ecommerce_engineered = feature_engineering_preprocessing(ecommerce_grouped, scaler)\n",
    "    engineered_features = [\"TotalCost\",\t\"TotalSales\", \"NumberOfOrders\", \"MeanMargin\"]\n",
    "    num_clusters = {\"Technology\" : 5, \"Furniture\" : 5, \"Office Supplies\" : 5}\n",
    "    #find_optimal_k(ecommerce_engineered, engineered_features)\n",
    "    #num_clusters = input(\"Enter optimal number of clusters for each category according to the plot.\")\n",
    "    #num_clusters = {key.strip(): int(value) for key, value in (item.split(\":\") for item in num_clusters.split(\", \"))}\n",
    "\n",
    "    cluster_product_data(ecommerce_engineered, num_clusters, engineered_features)\n",
    "    \n",
    "    return ecommerce_engineered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_clustering_pipeline(customer_clustering_data : dict) -> dict:\n",
    "    customer_clustering_data = calculate_total_revenue(customer_clustering_data)\n",
    "    clustering_features = [\"totalRevenue_PC_0\",\t\"totalRevenue_PC_1\",\t\"totalRevenue_PC_2\",\t\"totalRevenue_PC_3\",\t\"totalRevenue_PC_4\"]\n",
    "    num_clusters = {\"Technology\" : 5, \"Furniture\" : 5, \"Office Supplies\" : 5}\n",
    "    cluster_customer_data(customer_clustering_data, num_clusters, clustering_features)\n",
    "\n",
    "    return customer_clustering_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommended_price(complete_data : dict, customer_id : str, product_id : str):\n",
    "    percentile_data = {}\n",
    "    for category in complete_data.keys():\n",
    "        data = complete_data[category][[\"CustomerCluster\", \"Margin\"]]\n",
    "        data_array = np.array(list(data.itertuples(index = False, name = None)), \n",
    "                                 dtype=[(\"cluster_id\", int), (\"margin\", float)])\n",
    "        percentile_data[category] = data_array\n",
    "    \n",
    "    unsplit_complete_data = pd.concat(complete_data.values(), ignore_index=True)\n",
    "    category = unsplit_complete_data[unsplit_complete_data[\"ProductID\"] == product_id][\"Category\"].unique()[0]\n",
    "    data = complete_data[category]\n",
    "    cost = data[data[\"ProductID\"] == product_id][\"Cost\"].unique()[0]\n",
    "    \n",
    "    customer_cluster = data.loc[data[\"CustomerID\"] == customer_id, \"CustomerCluster\"].values[0]\n",
    "\n",
    "    margin_lower, margin_upper = get_lower_upper_margin(percentile_data[category], customer_cluster)\n",
    "    lower_price, upper_price = calculate_price(margin_lower, cost), calculate_price(margin_upper, cost)\n",
    "    recommended_price = calculate_price((margin_lower + margin_upper)/2, cost)\n",
    "\n",
    "    return lower_price, upper_price, recommended_price\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(customer_id, product_id):\n",
    "    filepath = 'ECOMMRecords2020.csv'\n",
    "    ecommerce_df = pd.read_csv(filepath)\n",
    "\n",
    "    ecommerce_df[\"Cost\"] = (ecommerce_df[\"Sales\"] - ecommerce_df[\"Profit\"])/ecommerce_df[\"Quantity\"] \n",
    "    ecommerce_df[\"Margin\"] = ecommerce_df[\"Profit\"]/ecommerce_df[\"Sales\"]\n",
    "\n",
    "    customer_clustering_data =  product_clustering_pipeline(ecommerce_df)\n",
    "    complete_data = customer_clustering_pipeline(customer_clustering_data)\n",
    "    lower_price, upper_price, recommended_price = compute_recommended_price(complete_data, customer_id, product_id)\n",
    "\n",
    "    print(f\"For product {product_id} and the customer {customer_id} we recommend the following:\")\n",
    "    print(f\"Lower sales price: {lower_price:.2f}\")\n",
    "    print(f\"Upper sales price: {upper_price:.2f}\")\n",
    "    print(f\"Recommended sales price: {recommended_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For product FUR-FU-10001215 and the customer GA-14725 we recommend the following:\n",
      "Lower sales price: 27.79\n",
      "Upper sales price: 35.74\n",
      "Recommended sales price: 31.27\n"
     ]
    }
   ],
   "source": [
    "pipeline(\"GA-14725\", \"FUR-FU-10001215\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
